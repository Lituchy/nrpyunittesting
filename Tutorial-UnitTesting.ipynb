{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Testing\n",
    "\n",
    "## Author: Kevin Lituchy\n",
    "\n",
    "### NRPy+ Source Code for this module: [UnitTesting/test_skeleton.py](../edit/UnitTesting/test_skeleton.py)\n",
    "( Not sure exactly what to put here ^ )\n",
    "\n",
    "## Introduction:\n",
    "The goal of this module is to give the user an overview/understanding of NRPy+'s Unit Testing framework, which will give the user enough information to begin creating their own unit tests. We will begin by giving an overview of the important prerequisite knowledge to make the most out of unit tests. Next, we give an explanation for the user interaction within the unit testing framework; this will give the user the ability to create tests for themselves. Then we give the user some insight into interpreting the output from their unit tests. Finally, a full example using a test module will be run through in full, both with and without errors, to give the user a realistic sense of what unit testing entails.\n",
    "\n",
    "For in-depth explanations of all subfunctions (not user-interactable), see the [README](../edit/UnitTesting/README.md). This may not be essential to get unit tests up and running, but it will be invaluable information if the user ever wants to make modifications to the unit testing code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc'></a>\n",
    "\n",
    "# Table of Contents\n",
    "$$\\label{toc}$$\n",
    "\n",
    "This module is organized as follows\n",
    "\n",
    "1. [Step 1](#motivation): Motivation and Prerequisite Knowledge\n",
    "    1. [Step 1.a](#dicts): Dictionaries\n",
    "    1. [Step 1.b](#logging): Logging\n",
    "1. [Step 2](#interaction): User interaction\n",
    "    1. [Step 2.a](#testfile): test_file\n",
    "    1. [Step 2.b](#trustedvaluesdict): trusted_values_dict\n",
    "    1. [Step 2.c](#bash): bash_script\n",
    "1. [Step 3](#output): Interpreting output\n",
    "1. [Step 4](#example): Full Example\n",
    "1. [Step 5](#latex_pdf_output): Output this module to $\\LaTeX$-formatted PDF\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='motivation'></a>\n",
    "\n",
    "# Step 1: Motivation and Prerequisite Knowledge \\[Back to [top](#toc)\\]\n",
    "$$\\label{motivation}$$\n",
    "\n",
    "What is the purpose of unit testing, and why should you do it? To begin\n",
    "thinking about that, consider what subtleties can occur within your code\n",
    "that are almost unnoticeable to the eye, but wind up giving you an\n",
    "incorrect result. You could make a small optimization, and not notice\n",
    "any change in your result. However, maybe the optimization you made only\n",
    "works on Python 3 and not Python 2, or it changes a value by some tiny\n",
    "amount -- too small to be noticeable at a simple glance, but enough to\n",
    "make a difference in succeeding calculations.\n",
    "\n",
    "This is where unit testing comes in. By initially calculating values for\n",
    "the globals of your modules in a **trusted** version of your code and\n",
    "storing those values in a dictionary, you can then easily check if\n",
    "something stopped working correctly by comparing your newly calculated\n",
    "values to the ones you've stored. On the frontend, there are four\n",
    "concepts essential to understand to get your unit tests up and running:\n",
    "`trusted_values_dict`, `create_test`, your testing module (which\n",
    "will simply be referred to as `test_file`), and a bash script (which\n",
    "will simply be referred to as `bash_script`). The usage of each of these\n",
    "modules is outlined in the Beginning section. There is also some\n",
    "important prerequisite knowledge that may be helpful to grasp before\n",
    "beginning your testing. There are many functions at play in the backend\n",
    "as well, all of which will be described in detail below in the\n",
    "Functions section. Mastery of these functions may not be\n",
    "essential to get your tests up-and-running, but some basic understanding\n",
    "of them with undoubtedly help with debugging.\n",
    "\n",
    "An important caveat is that the unit testing does not test the\n",
    "**correctness** of your code or your variables. The unit tests act as a\n",
    "protective measure to ensure that nothing was broken between versions of\n",
    "your code; it gets its values by running _your_ code, so if something\n",
    "starts out incorrect, it will be stored as incorrect in the system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prereq'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dicts'></a>\n",
    "\n",
    "## Step 1.a: Dictionaries \\[Back to [top](#toc)\\]\n",
    "$$\\label{dicts}$$\n",
    "\n",
    "Dictionaries are used throughout the unit testing infrastructure. The user must create simple dictionaries to pass into our testing functions. If you know nothing about dictionaries, we recommend [this](https://www.w3schools.com/python/python_dictionaries.asp) article; it will get you up to speed for simple dictionary creation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='logging'></a>\n",
    "\n",
    "## Step 1.b: Logging \\[Back to [top](#toc)\\]\n",
    "$$\\label{logging}$$\n",
    "\n",
    "Logging is a python module that allows the user to specify their desired level of output by modifying a parameter, rather than having to use if-statements and print-statements. We allow the user to change the level of output through a parameter `logging_level`, in which we support the following levels:\n",
    "\n",
    "`ERROR`: Only print when an error occurs\n",
    "\n",
    "`INFO`: Print general information about test beginning, completion, major function calls, etc., as well as everything above. (Recommended)\n",
    "\n",
    "`DEBUG`: Print maximum amount of information -- every comparison, as well as everything above.\n",
    "\n",
    "A good way to think of these logging levels is that `INFO` is the default, `ERROR` is similar to a non-verbose mode, and `DEBUG` is similar to a verbose mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='interaction'></a>\n",
    "\n",
    "# Step 2: User Interaction \\[Back to [top](#toc)\\]\n",
    "$$\\label{interaction}$$\n",
    "\n",
    "Within the module the user is intending to test, a directory named `tests` should be created. This will house the test file for the given module and its associated `trusted_values_dict`. For example, if I intend to test `BSSN`, I will create a new directory `BSSN/tests`. Within the `tests` directory, the user should create a file called `test_(module).py` -- or `test_BSSN.py` with the given example. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='testfile'></a>\n",
    "\n",
    "## Step 2.a: Test File \\[Back to [top](#toc)\\]\n",
    "$$\\label{testfile}$$\n",
    "\n",
    "The test file is how the user inputs their module, functions, and globals information to the testing suite. For the purpose of consistency, we've created a skeleton for the test file (found [here](../edit/UnitTesting/test_skeleton.py)) that contains all information the user must specify. The user should change the name of the function to something relevant to their test. However, note that the name of the function must begin with `test_` in order for the bash script to successfully run the test -- this is the default naming scheme for most test suites/software. Inside the function, multiple fields are required to be filled out by the user; these fields are `module`, `module_name`, and `function_and_global_dict`. Below the function there is some code that begins with `if __name__ == '__main__':`. The user can ignore this code as it does backend work and makes sure to pass the proper information for the test.\n",
    "\n",
    "`module` is a string representing the module to be tested. \n",
    "\n",
    "`module_name` is how the user would like the data acquired from testing `module` to be represented.\n",
    "\n",
    "`function_and_global_dict` is a dictionary whose keys are string representations of functions that the user would like to be called on `module` and whose values are lists of globals that can be acquired by running their respective functions on `module`\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "def test_BrillLindquist():\n",
    "\n",
    "    module = 'BSSN.BrillLindquist'\n",
    "    \n",
    "    module_name = 'bl'\n",
    "    \n",
    "    function_and_global_dict = {'BrillLindquist(ComputeADMGlobalsOnly = True)': \n",
    "                                ['alphaCart', 'betaCartU', 'BCartU', 'gammaCartDD', 'KCartDD']}\n",
    "                                \n",
    "    create_test(module, module_name, function_and_global_dict)\n",
    "```\n",
    "\n",
    "In most cases, this simple structure is enough to do exactly what the user wants. Sometimes, however, there is other information that needs to be passed into the test -- this is where optional arguments come in.\n",
    "\n",
    "The tests can take two optional arguments, `logging_level` and `initialization_string_dict`\n",
    "\n",
    "`logging_level` follows the same scheme as described [above](#logging). \n",
    "\n",
    "`initialization_string_dict` is a dictionary whose keys are functions that **must** also be in `function_and_global_dict` and whose values are strings containing well-formed Python code. The strings are executed as Python code before its respective function is called on the module. The purpose of this argument is to allow the user to do any necessary NRPy+ setup before the call their function.\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "def test_quantities():\n",
    "\n",
    "    module = 'BSSN.BSSN_quantities'\n",
    "\n",
    "    module_name = 'BSSN_quantities'\n",
    "    \n",
    "    function_and_global_dict = {'BSSN_basic_tensors()': ['gammabarDD', 'AbarDD', 'LambdabarU', 'betaU', 'BU']}\n",
    "                                \n",
    "    logging_level = 'DEBUG'\n",
    "    \n",
    "    initialization_string = '''\n",
    "import reference_metric as rfm\n",
    "rfm.reference_metric()\n",
    "rfm.ref_metric__hatted_quantities()\n",
    "'''\n",
    "    \n",
    "    initialization_string_dict = {'BSSN_basic_tensors()': initialization_string}\n",
    "                                \n",
    "    create_test(module, module_name, function_and_global_dict, logging_level=logging_level,          \n",
    "                initialization_string_dict=initialization_string_dict)\n",
    "```\n",
    "\n",
    "An important thing to note is that even though `initialization_string` looks odd with its indentation, this is necessary for Python to interpret it correctly. If it was indented, Python would think you were trying to indent that code when it shouldn't be, and an error will occur.\n",
    "\n",
    "A question you may be wondering is why we need to create a new dictionary for the intialization string, insted of just passing it as its own argument. This is because the testing suite can accept multiple function calls, each with their own associated global list, in one function. It then naturally follows that we need `initailization_string_dict` to allow each function call to have its own code that runs before its function call. In the following example, the function `BSSN_basic_tensors()` has an initialization string, but the function `declare_BSSN_gridfunctions_if_not_declared_already()` doesn't. You can also clearly see they each have their own associated globals.\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "def test_quantities():\n",
    "\n",
    "    module = 'BSSN.BSSN_quantities'\n",
    "\n",
    "    module_name = 'BSSN_quantities'\n",
    "    \n",
    "    function_and_global_dict = {'declare_BSSN_gridfunctions_if_not_declared_already()': \n",
    "                                ['hDD', 'aDD', 'lambdaU', 'vetU', 'betU', 'trK', 'cf', 'alpha'], \n",
    "                                \n",
    "                                'BSSN_basic_tensors()': ['gammabarDD', 'AbarDD', 'LambdabarU', 'betaU', 'BU']}\n",
    "                                \n",
    "    logging_level = 'DEBUG'\n",
    "    \n",
    "    initialization_string = '''\n",
    "import reference_metric as rfm\n",
    "rfm.reference_metric()\n",
    "rfm.ref_metric__hatted_quantities()\n",
    "'''\n",
    "    \n",
    "    initialization_string_dict = {'BSSN_basic_tensors()': initialization_string}\n",
    "                                \n",
    "    create_test(module, module_name, function_and_global_dict, logging_level=logging_level,          \n",
    "                initialization_string_dict=initialization_string_dict)\n",
    "```\n",
    "\n",
    "Lastly, within a single test file, you can define multiple test functions. It's as simple as defining a new function whose name starts with `test_` in the file and making sure to fill out the necessary fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='trustedvaluesdict'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='trustedvaluesdict'></a>\n",
    "\n",
    "## Step 2.b: trusted_values_dict \\[Back to [top](#toc)\\]\n",
    "$$\\label{trustedvaluesdict}$$\n",
    "\n",
    "At this point, it's hopefully understood that our test suite will compare trusted values of your variables to newly calculated values to ensure that no variables were unknowingly modified. The `trusted_values_dict` acts as the means of storing the trusted value of each variable with the purpose of future comparison. A new `trusted_values_dict` is created by default when a test file is run for the first time -- it's visible in `tests/trusted_values_dict.py`. Note that if you run your code but can't see the file, refresh your IDE -- it's there, sometimes IDE's just get confused when you create a file within Python. The default structure of all `trusted_value_dict` files is as follows:\n",
    "\n",
    "```\n",
    "from mpmath import mpf, mp, mpc\n",
    "from UnitTesting.standard_constants import precision\n",
    "\n",
    "mp.dps = precision\n",
    "trusted_values_dict = {}\n",
    "\n",
    "```\n",
    "\n",
    "The proper code to copy into this file will be printed to the console when a test is run. The test suite will also automatically write its calculated globals' values for a given function to this file in the proper format; make sure to check that things seem correct though! Remember that the `trusted_values_dict` stores **trusted**, not necessarily **correct** values for each global."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bash'></a>\n",
    "\n",
    "## Step 2.c: Bash Script \\[Back to [top](#toc)\\]\n",
    "$$\\label{bash}$$\n",
    "\n",
    "In order to successfully run all the user's unit tests and properly integrate testing with TravisCI, we use a bash script as the 'hub' of all the tests to be run. This makes it easy for the user to comment out tests they don't want to run, add new tests to be automatically run with one line, etc. \n",
    "\n",
    "We offer a skeleton file, [`run_NRPy_UnitTests_skeleton`](../edit/UnitTesting/run_NRPy_UnitTests_skeleton.sh), which contains all the proper code to be easily run with minimum user interaction. All the user must do is call the `add_test` function on the test file they'd like to be run under the `TODO` comment. For example, `add_test BSSN/tests/test_BSSN.py` would run the `BSSN` tests. Then to add more tests, simply go to the next line and add another test. It's as simple as that! \n",
    "\n",
    "To run the bash script, open up a terminal, type in the path of the bash script, and then pick the Python interpreter to run the code -- for example, `./UnitTesting/run_NRPy_UnitTests.sh python` or `./UnitTesting/run_NRPy_UnitTests.sh python3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='output'></a>\n",
    "\n",
    "# Step 3: Interpreting Output \\[Back to [top](#toc)\\]\n",
    "$$\\label{output}$$\n",
    "\n",
    "Once a user's tests are fully set up, they need to be able to interpret the output of their tests; doing this allows the user to easily figure out what went wrong, why, and how to fix it. The amount of output for a given module is of course dependent on its logging level\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='latex_pdf_output'></a>\n",
    "\n",
    "# Step 5: Output this module to $\\LaTeX$-formatted PDF file \\[Back to [top](#toc)\\]\n",
    "$$\\label{latex_pdf_output}$$\n",
    "\n",
    "The following code cell converts this Jupyter notebook into a proper, clickable $\\LaTeX$-formatted PDF file. After the cell is successfully run, the generated PDF may be found in the root NRPy+ tutorial directory, with filename [Tutorial-UnitTesting.pdf](Tutorial-UnitTesting.pdf) (Note that clicking on this link may not work; you may need to open the PDF file through another means.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Tutorial-UnitTesting.ipynb to latex\n",
      "[NbConvertApp] Writing 36394 bytes to Tutorial-UnitTesting.tex\n",
      "This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017/Debian) (preloaded format=pdflatex)\n",
      " restricted \\write18 enabled.\n",
      "entering extended mode\n",
      "This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017/Debian) (preloaded format=pdflatex)\n",
      " restricted \\write18 enabled.\n",
      "entering extended mode\n",
      "This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017/Debian) (preloaded format=pdflatex)\n",
      " restricted \\write18 enabled.\n",
      "entering extended mode\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to latex --template latex_nrpy_style.tplx Tutorial-UnitTesting.ipynb\n",
    "!pdflatex -interaction=batchmode Tutorial-UnitTesting.tex\n",
    "!pdflatex -interaction=batchmode Tutorial-UnitTesting.tex\n",
    "!pdflatex -interaction=batchmode Tutorial-UnitTesting.tex\n",
    "!rm -f Tut*.out Tut*.aux Tut*.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
